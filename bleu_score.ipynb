{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anantraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1840735921322381"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "def aggregate_bleu_score(refs, sys, n=2):\n",
    "    \n",
    "    p_ns = np.zeros(n)\n",
    "    c = 0  \n",
    "    r = 0  \n",
    "\n",
    "    for predicted in sys:\n",
    "        predicted_tokens = word_tokenize(predicted)\n",
    "        reference_tokens = [word_tokenize(ref[0]) for ref in refs]\n",
    "\n",
    "        c += len(predicted_tokens)\n",
    "        r += min([len(target) for target in reference_tokens], key=lambda x: abs(x - len(predicted_tokens)))\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            pred_ngrams = list(ngrams(predicted_tokens, i, pad_right=True, right_pad_symbol=None))\n",
    "            max_ref_ngrams = {}\n",
    "            for ref_tokens in reference_tokens:\n",
    "\n",
    "                ref_ngrams = list(ngrams(ref_tokens, i, pad_right=True, right_pad_symbol=None))\n",
    "                ref_ngram_counts = Counter(ref_ngrams)\n",
    "                for ngram in ref_ngram_counts:\n",
    "                    if ngram in max_ref_ngrams:\n",
    "                        max_ref_ngrams[ngram] = max(max_ref_ngrams[ngram], ref_ngram_counts[ngram])\n",
    "                    else:\n",
    "                        max_ref_ngrams[ngram] = ref_ngram_counts[ngram]\n",
    "\n",
    "            clipped_count = sum(min(count, max_ref_ngrams.get(ngram, 0)) for ngram, count in Counter(pred_ngrams).items())\n",
    "            total_count = len(pred_ngrams)\n",
    "            p_ns[i - 1] += clipped_count / total_count if total_count > 0 else 0\n",
    "\n",
    "    #print('p_ns',p_ns)\n",
    "    p_ns = p_ns / len(sys)  # Average precision per n-gram \n",
    "    brevity_penalty = np.exp(1 - r / c) if c < r else 1\n",
    "\n",
    "    bleu_score = brevity_penalty * np.exp(sum(np.log(p) for p in p_ns) / n)\n",
    "    return bleu_score\n",
    "\n",
    "# # Example usage with your provided input\n",
    "refs = [['Sì, verrete in'],\n",
    "        ['Siete ferito, signore?']]\n",
    "sys = ['spietate stabilirmi ',\n",
    "       'Siete spietate anticamera signore ?']\n",
    "\n",
    "# refs = [['Alice is'],['She loves UK more than states']]\n",
    "# sys = ['Alice loves UK','US ']\n",
    "\n",
    "bs = aggregate_bleu_score(refs, sys)\n",
    "bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1663292666623517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/anantraj/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def dynamic_aggregate_bleu_score(refs, sys, max_n=4):\n",
    "    \n",
    "    p_ns = np.zeros(max_n)\n",
    "    c = 0  # Total length of system outputs\n",
    "    r = 0  # Total length of the closest reference lengths\n",
    "\n",
    "    for predicted in sys:\n",
    "        predicted_tokens = word_tokenize(predicted)\n",
    "        reference_tokens = [word_tokenize(ref[0]) for ref in refs]\n",
    "\n",
    "        c += len(predicted_tokens)\n",
    "        r += min([len(target) for target in reference_tokens], key=lambda x: abs(x - len(predicted_tokens)))\n",
    "\n",
    "        # Find the smallest sentence length among the current predicted sentence and all references\n",
    "        min_length = min([len(predicted_tokens)] + [len(ref) for ref in reference_tokens])\n",
    "\n",
    "        # Determine the highest n-gram order that makes sense given the sentence lengths\n",
    "        effective_n = min(max_n, min_length)\n",
    "\n",
    "        for i in range(1, effective_n + 1):\n",
    "            pred_ngrams = list(ngrams(predicted_tokens, i, pad_right=True, right_pad_symbol=None))\n",
    "            max_ref_ngrams = {}\n",
    "\n",
    "            for ref_tokens in reference_tokens:\n",
    "                ref_ngrams = list(ngrams(ref_tokens, i, pad_right=True, right_pad_symbol=None))\n",
    "                ref_ngram_counts = Counter(ref_ngrams)\n",
    "                for ngram in ref_ngram_counts:\n",
    "                    max_ref_ngrams[ngram] = max(max_ref_ngrams.get(ngram, 0), ref_ngram_counts[ngram])\n",
    "\n",
    "            clipped_count = sum(min(count, max_ref_ngrams.get(ngram, 0)) for ngram, count in Counter(pred_ngrams).items())\n",
    "            total_count = len(pred_ngrams)\n",
    "            p_ns[i - 1] += clipped_count / total_count if total_count > 0 else 0\n",
    "\n",
    "    # Adjust p_ns for the number of sentences processed\n",
    "    p_ns = p_ns / len(sys)  # Average precision per n-gram across all sentences considered\n",
    "    brevity_penalty = np.exp(1 - r / c) if c < r else 1\n",
    "\n",
    "    # Compute BLEU score using only the effective n-grams\n",
    "    bleu_score = brevity_penalty * np.exp(sum(np.log(p) for p in p_ns[:effective_n]) / effective_n)\n",
    "    return bleu_score\n",
    "\n",
    "# Example usage with your provided input\n",
    "refs = [['Sì, verrete in'],\n",
    "        ['Siete ferito, signore?']]\n",
    "sys = ['spietate stabilirmi ',\n",
    "       'Siete spietate anticamera signore ?']\n",
    "\n",
    "# refs = [['Alice is'],['She loves UK more than states']]\n",
    "# sys = ['Alice loves UK','US ']\n",
    "bs = dynamic_aggregate_bleu_score(refs, sys)\n",
    "print(bs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ttds_cw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
